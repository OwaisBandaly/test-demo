# ==========================================================
# ðŸ§  MACHINE LEARNING ASSIGNMENT (Q5Aâ€“8B)
# ==========================================================
# Name: _______________________
# Roll No: ____________________
# Subject: Machine Learning
# ==========================================================

# Common Imports
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# ==========================================================
# 5A) NaÃ¯ve Bayes Classifier (Categorical Data)
# ==========================================================
from sklearn.naive_bayes import CategoricalNB

print("\n===== 5A) NaÃ¯ve Bayes Classifier (Categorical Data) =====")

data = {
    'Age Group': ['Young', 'Young', 'Middle', 'Young', 'Middle', 'Senior', 'Senior', 'Middle', 'Young', 'Middle'],
    'Gender': ['Male', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Female'],
    'Class': ['Not Buy', 'Buy', 'Not Buy', 'Not Buy', 'Buy', 'Not Buy', 'Buy', 'Buy', 'Not Buy', 'Buy']
}

df = pd.DataFrame(data)

# Label Encoding
le_age = LabelEncoder()
le_gender = LabelEncoder()
le_class = LabelEncoder()

df['Age Group'] = le_age.fit_transform(df['Age Group'])
df['Gender'] = le_gender.fit_transform(df['Gender'])
df['Class'] = le_class.fit_transform(df['Class'])

# Features and Target
X = df[['Age Group', 'Gender']]
y = df['Class']

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Model
model = CategoricalNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Results
print("Predictions:", le_class.inverse_transform(y_pred))
print("Actual:", le_class.inverse_transform(y_test))
print("Accuracy:", round(accuracy_score(y_test, y_pred), 2))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# ==========================================================
# 5B) Gaussian NaÃ¯ve Bayes Classifier (Loan Dataset)
# ==========================================================
from sklearn.naive_bayes import GaussianNB

print("\n===== 5B) Gaussian NaÃ¯ve Bayes (Loan Dataset) =====")

df = pd.read_csv("loan.csv")

# Handle missing values
for col in df.columns:
    if df[col].dtype == 'object':
        df[col] = df[col].fillna(df[col].mode()[0])
    else:
        df[col] = df[col].fillna(df[col].mean())

# Features and Target
X = df.drop('Loan_Status', axis=1)
y = df['Loan_Status']

# Encode categorical features
le = LabelEncoder()
for col in X.columns:
    if X[col].dtype == 'object':
        X[col] = le.fit_transform(X[col])

y = LabelEncoder().fit_transform(y.astype(str))

# Split and Train
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Evaluation
print("Accuracy:", round(accuracy_score(y_test, y_pred), 2))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

# ==========================================================
# 6) K-Means Clustering Algorithm
# ==========================================================
from sklearn.cluster import KMeans

print("\n===== 6) K-Means Clustering =====")

df = pd.read_csv("housing.csv")

# Encode if needed
if 'ocean_proximity' in df.columns:
    df['ocean_proximity'] = LabelEncoder().fit_transform(df['ocean_proximity'])

# Numeric columns only
df_num = df.select_dtypes(include=[np.number]).fillna(df.mean(numeric_only=True))

# Apply K-Means
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
labels = kmeans.fit_predict(df_num)

print("Cluster Labels (first 10):", labels[:10])

# ==========================================================
# 7) Support Vector Machine (SVM)
# ==========================================================
from sklearn.svm import SVC

print("\n===== 7) Support Vector Machine =====")

data = pd.read_csv("data.csv")
data = data.drop(['id', 'Unnamed: 32'], axis=1, errors='ignore')

# Fill missing values
for col in data.columns:
    if data[col].dtype == 'object':
        data[col] = data[col].fillna(data[col].mode()[0])
    else:
        data[col] = data[col].fillna(data[col].median())

# Encode categorical
for col in data.select_dtypes(include='object').columns:
    data[col] = LabelEncoder().fit_transform(data[col])

# Split
X = data.drop('diagnosis', axis=1)
y = data['diagnosis']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Model
model = SVC(kernel='rbf', random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# Evaluate
print("Accuracy:", round(accuracy_score(y_test, y_pred) * 100, 2), "%")
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, cmap="Blues", fmt="d")
plt.title("SVM Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

# ==========================================================
# 8A) Ensemble Learning â€“ Bagging (Random Forest)
# ==========================================================
from sklearn.ensemble import RandomForestClassifier

print("\n===== 8A) Ensemble Learning (Bagging - Random Forest) =====")

data = pd.read_csv("loan_data.csv").dropna()

# Encode categorical
for col in data.select_dtypes(include='object').columns:
    data[col] = LabelEncoder().fit_transform(data[col])

X = data.drop('not.fully.paid', axis=1)
y = data['not.fully.paid']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

print("Accuracy:", round(accuracy_score(y_test, y_pred), 2))
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

# ==========================================================
# 8B) Ensemble Learning â€“ Boosting & Blending
# ==========================================================
from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier

print("\n===== 8B) Ensemble Learning (Boosting & Blending) =====")

data = pd.read_csv("loan_data.csv").dropna()

# Encode
for col in data.select_dtypes(include='object').columns:
    data[col] = LabelEncoder().fit_transform(data[col])

X = data.drop('not.fully.paid', axis=1)
y = data['not.fully.paid']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# AdaBoost
ada = AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=42)
ada.fit(X_train, y_train)
y_pred_ada = ada.predict(X_test)
print("AdaBoost Accuracy:", round(accuracy_score(y_test, y_pred_ada), 2))

# Gradient Boosting
gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)
gb.fit(X_train, y_train)
y_pred_gb = gb.predict(X_test)
print("Gradient Boosting Accuracy:", round(accuracy_score(y_test, y_pred_gb), 2))

# Random Forest (for blending)
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Blending
rf_pred = rf.predict_proba(X_test)[:, 1]
ada_pred = ada.predict_proba(X_test)[:, 1]
gb_pred = gb.predict_proba(X_test)[:, 1]

final_pred = (rf_pred + ada_pred + gb_pred) / 3
final_pred_label = np.where(final_pred > 0.5, 1, 0)

print("\nBlending (RF + AdaBoost + GB) Accuracy:", round(accuracy_score(y_test, final_pred_label), 2))
print("Classification Report:\n", classification_report(y_test, final_pred_label))
print("Confusion Matrix:\n", confusion_matrix(y_test, final_pred_label))