5A) A) Write a program to implement the Naïve Bayes Claassifier on the given dataset.
B) Write a program to implement the Gaussian  Naive Bayes Claassifier on the given dataset(Loan Dataset).

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import CategoricalNe
from sklearn.model_selection import train_test_split

from sklearn.metrics import accuracy score, confusion_matrix

data(

}

'Age Group': ['Young', 'Young", "Middle', 'Young', 'Middle', 'Senior', 'Senior", "Middle", "Young', 'Middle"],

"Gender":

"Male', 'Female', 'Male', 'Male', 'Female', 'Female', 'Male', 'Male', 'Female', 'Female"],

'Class':

['Not Buy', 'Buy', 'Not Buy", "Not Buy', 'Buy", "Not Buy", "Buy', 'Buy", "Not Buy", "Buy"]

df= pd.DataFrame(data)

le_age LabelEncoder()

le_gender LabelEncoder()

le class LabelEncoder()

df['Age Group'] le_age.fit_transform(df['Age Group'])

df Gender'] le_gender.fit_transform(df['Gender'])

df ['Class']

le_class.fit_transform(df['Class'])

Features and Target

x= df[['Age Group', 'Gender']]

y = df['Class')

#Train-Test split

x_train, x_test, y_train, y_test train_test_split(x, y, test size=0.3, random_state=42)

#Naive Bayes Model

model CategoricalNB()

model.fit(X_train, y_train)

#Predictions

y_pred model.predict(X_test)

#Results

print("Predictions:", le_class.inverse_transform(y_pred))

print("Actual:", le_class.inverse_transform(y_test))

print("Accuracy:", accuracy_score(y_test, y_pred))

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))



5B) 
import pandas as pd

from sklearn.model_selection import train_test_split

from sklearn.preprocessing import LabelEncoder

from sklearn.naive_bayes import GaussianNB

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

from sklearn.impute import SimpleImputer

#1. Load dataset'

df pd.read_csv("loan loan.csv")

#2. Separate features and target (assuming 'Loan Status' is target)

x = df.drop('Loan_Status', axis=1)

y = df['Loan_Status']

#3. Handle missing values (robust version)

for col in X.columns:

if X[col].dtype == 'object':

#if column is categorical (string), fill with mode

if x[col].isnull().sum() > 0: #only if NaN exists X[col].fillna(x[col].mode()[0], inplace=True)

else:

#if column is numeric, fill with mean

if x[col].isnull().sum() > 0:

X[col].fillna(X[col].mean(), inplace=True)

#4. Encode categorical features

for col in X.columns:

if x[col].dtype == 'object':

le LabelEncoder()
Y = le_target.fit_transform(y.astype(str))
Print(X.isnull().sum())
Print(X.dtypes)
x, y, test_size=0.3, random_state=42

)

I

#8. Gaussian Naive Bayes model

model GaussianNB()

model.fit(X_train, y_train)

GaussianNB

GaussianNB()

#9. Predictions

y_pred model.predict(X_test)

#10. Evaluation

print("Accuracy:", accuracy_score(y_test, y_pred))

print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

print("Classification Report:\n", classification_report(y_test, y_pred))



6) Write a program to implement the K - Means Algorithm on the given dataset.
import pandas as pd

from sklearn.preprocessing import LabelEncoder from sklearn.cluster import KMeans

白

#Load dataset

file_path 'housing housing.csv'

dfpd.read_csv(file_path)

I

#Handle categorical column ('ocean_proximity')

if 'ocean_proximity' in df.columns:

le LabelEncoder()

df['ocean_proximity'] = le.fit_transform(df['ocean_proximity'])

#Keep only numeric columns for clustering

df_num= df.select_dtypes(include=['float64', 'int64'])

df_num= df_num.fillna(df_num.mean())

# Apply K-Means

k4 Number of clusters (modifiable)

kmeans KMeans(n_clusters=k, random_state=42)

labels kmeans.fit_predict(df_num)



7) Use Support Vector Machine to solve real-world problem.
# Import libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.svm import SV
seaborn as sns
import matplotlib.pyplot as plt
# Load dataset
data = pd.read_csv("data.csv")
print("\nOriginal Shape:", data.shape)
print("\nColumns:", list(data.columns))
# Drop unnecessary columns (id and empty columns)
data = data.drop(['id', 'Unnamed: 32'], axis=1, errors='ignore')
# Fill missing values
for col in data.columns:
if data[col].dtype == 'object':
data[col] = data[col].fillna(data[col].mode()[0]) # fill categorical NaN with mode
else:
data[col] = data[col].fillna(data[col].median()) # fill numeric NaN with median
print("\nAfter Filling Missing Values Shape:", data.shape)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report import
# Encode categorical variables
le = LabelEncoder()
for col in data.columns:
if data[col].dtype == 'object':
data[col] = le.fit_transform(data[col].astype(str))
print("\nEncoded Data Sample:\n", data.head())
# Split into features (X) and target (y)
X = data.drop('diagnosis', axis=1)
y = data['diagnosis']
print("\nFeature sample:\n", X.head())
print("\nTarget sample:\n", y.head())

# Split into training/testing data
X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=42)
# Scale features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# Train Support Vector Machine
model = SVC(kernel='rbf', random_state=42)
model.fit(X_train, y_train)
# Predict
y_pred = model.predict(X_test)
# Evaluate
print("\nAccuracy:", round(accuracy_score(y_test, y_pred)*100, 2), "%")
print("\nClassification Report:\n", classification_report(y_test, y_pred))
# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, cmap="Blues", fmt="d")
plt.title("SVM Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()



8A) Use Ensemble learning (Bagging)  solve real-world problem.         
B) Use Ensemble learning (Boosting/Blending)  solve real-world problem.


import pandas as pd
from sklearn.model selection import train test_split from sklearn.ensemble import Random ForestClassifier from sklearn.metrics import accuracy_score, classification_report, confusion matrix from sklearn.preprocessing import LabelEncoder

#Load dataset

data pd.read_csv("loan_data.csv")

#Handle missing values (if any)

data data.dropna()

#Encode categorical features (like 'purpose')

le LabelEncoder()

for col in data.select_dtypes (include='object').columns: data[col] le.fit_transform(data[col])

#Split features and target

X data.drop('not.fully.paid', axis=1)

y data['not.fully.paid"]

#Train-test split

x_train, x_test, y_train, y_test train_test_split(x, y, test size-0.2, random state-42)

#Bagging Model: Random Forest

rf Random ForestClassifier(n_estimators 100, random_state=42)

rf.fit(x_train, y_train)

#Predictions

y_pred rf.predict(x_test)

# Evaluation

print(" Random Forest (Bagging) Accuracy:", accuracy_score(y_test, y_pred)) print("\nClassification Report:\n", classification_report(y_test, y_pred))

print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))


8B)
8B)
Import pandas as pd

from-sklearn.model_selection import train test_split

from sklearn.preprocessing import LabelEncoder

from sklearn.metrics import accuracy score, classification_report, confusion_matrix

from sklearn.ensemble import AdafloostClassifier, GradientBoostingClassifier, Random ForestClassifier

Import numpy as np

#Load data

data pd.read_csv("loan_data.csv")

data data.dropna()

Encode categorical variable

le LabelEncoder()

for col in data.select_dtypes (include='object').columns:

X

y

data[col] le.fit_transform(data[col])

Split features and target

data.drop('not.fully.paid', axis-1)

data['not.fully.paid"]

#Train-test split

x_train, x_test, y_train, y_test train test split(x, y, test size-0.2, random state-42)

Adatioost

ada AdaBoostClassifier(n_estimators-100, learning rate-0.5, random_state-42)

ada.fit(x_train, y train)

y_pred_ada ada.predict(x_test)

print(" Adalboost Results")

print("Accuracy:", accuracy_score(y_test, y_pred_ada))

print("Classification Report:\n", classification_report(y_test, y_pred_ada)) print("Confusion Matrix:\n", confusion matrix(y_test, y_pred_ada))

......... Gradient Boosting

gb GradientBoostingClassifier(n_estimators-100, learning rate-0.1, random_state-42)

gb.fit(X_train, y train) y_pred gbgb.predict(X_test)

print("\n Gradient Boosting Results")

print("Accuracy:", accuracy_score(y_test, y pred_gb))

print("Classification Report:\n", classification_report(y_test, y_pred_gb))

print("Confusion Matrix:\n", confusion matrix(y_test, y_pred_gb))
rf RandomForestClassifier(n_estimators=100, random_state=42)

ada AdaBoostClassifier(n_estimators=100, learning_rate=0.5, random_state=42)

gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)

rf.fit(X_train, y_train)

ada.fit(X_train, y_train)

gb.fit(X_train, y_train)

#Get predictions from each model (probabilities)

rf_pred rf.predict_proba(X_test) [:, 1]

ada_pred ada.predict_proba(x_test) [:, 1]

gb_pred gb.predict_proba(x_test)[:, 1]

#Simple average (blending)

final_pred (rf_pred + ada_pred + gb_pred) / 3

final_pred_label = np.where(final_pred > 0.5, 1, 0)

print("\n Blending (RF + AdaBoost + GB) Results")

print("Accuracy:", accuracy_score(y_test, final_pred_label))

print("Classification Report:\n", classification_report(y_test, final_pred_label))

print("Confusion Matrix:\n", confusion_matrix(y_test, final_pred_label))
